import abc
import os

import anthropic
import ollama
import openai
import together
import torch
from PIL import Image
from tenacity import retry, stop_after_attempt, wait_fixed
from transformers import AutoProcessor, MllamaForConditionalGeneration

import files
from data import ContentImageMessage, ContentTextMessage, Conversation


class LLMChat(abc.ABC):
    SUPPORTED_LLM_NAMES: list[str] = []

    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        model_path: str | None = None,
    ):
        """
        Initialize the LLM chat object.

        Args:
            model_name (str): The model name to use.
            max_retries (int): Max number of API calls allowed before giving up.
            wait_seconds (int): Number of seconds to wait between API calls.
            temperature (float): Temperature parameter for sampling.
            seed (int): Seed for random number generator, passed to the model if applicable.
            model_path (Optional[str]): Local path to the model, defaults to None.
        """
        assert (
            model_name in self.SUPPORTED_LLM_NAMES
        ), f"Model name {model_name} must be one of {self.SUPPORTED_LLM_NAMES}."
        self.model_name = model_name
        self.max_retries = max_retries
        self.wait_seconds = wait_seconds
        self.temperature = temperature
        self.seed = seed
        self.model_path = model_path

        self.model_kwargs = dict(
            max_retries=max_retries,
            wait_seconds=wait_seconds,
            temperature=temperature,
            seed=seed,
            model_path=model_path,
        )

    @abc.abstractmethod
    def generate_response(self, conv: Conversation) -> str:
        """
        Generate a response for the conversation.

        Args:
            conv (Conversation): The conversation object.

        Returns:
            str: The response generated by the model.
        """
        pass


class CommonLLMChat(LLMChat):
    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        model_path: str | None = None,
    ):
        super().__init__(model_name, max_retries, wait_seconds, temperature, seed, model_path)
        self.client = None

    @abc.abstractmethod
    def _call_api(self, messages_api_format: list[dict]) -> str:
        """
        Expects messages ready for API. Use `convert_conv_to_api_format` to convert a conversation
        into such a list.
        """
        pass

    def _convert_conv_to_api_format(self, conv: Conversation) -> list[dict]:
        """
        Converts the conversation object to a common format supported by various LLM providers.
        Common format is:
        ```
        [
            {"role": role1, "content": [{"type": "text", "text": text1},
            {"role": role2, "content": [{"type": "text", "text": text2},
            {"role": role3, "content": [{"type": "text", "text": text3},
        ]
        ```
        """
        formatted_messages = []
        for message in conv.messages:
            for content in message.content:
                match content:
                    case ContentTextMessage(text=text):
                        formatted_messages.append({"role": message.role, "content": [{"type": "text", "text": text}]})
                    case ContentImageMessage(image=image):
                        base64_image = files.pil_to_base64(image)
                        formatted_messages.append(
                            {
                                "role": message.role,
                                "content": [
                                    {
                                        "type": "image_url",
                                        "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                                    }
                                ],
                            }
                        )
        return formatted_messages

    def generate_response(self, conv: Conversation) -> str:
        """
        Generate response for the conversation.
        """
        assert self.client is not None, "Client is not initialized."

        # max_retries and wait_seconds are object attributes, and cannot be written around the generate_response function
        # So we need to wrap the _call_api function with the retry decorator
        @retry(stop=stop_after_attempt(self.max_retries), wait=wait_fixed(self.wait_seconds))
        def _call_api_wrapper(conv: Conversation) -> str:
            messages_api_format: list[dict] = self._convert_conv_to_api_format(conv)
            return self._call_api(messages_api_format)

        return _call_api_wrapper(conv)


class OpenAIChat(CommonLLMChat):
    SUPPORTED_LLM_NAMES: list[str] = [
        "gpt-4o-mini-2024-07-18",
        "gpt-4o-2024-11-20",
    ]

    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        model_path: str | None = None,
    ):
        super().__init__(model_name, max_retries, wait_seconds, temperature, seed, model_path)
        self.client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def _call_api(self, messages_api_format: list[dict]) -> str:
        # https://platform.openai.com/docs/api-reference/introduction
        completion = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages_api_format,
            temperature=self.temperature,
            seed=self.seed,
        )
        return completion.choices[0].message.content


class TogetherChat(OpenAIChat):
    SUPPORTED_LLM_NAMES: list[str] = [
        "together:meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
        "together:meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
        "together:meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
        "together:meta-llama/Llama-Vision-Free",
    ]

    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        model_path: str | None = None,
    ):
        assert model_name.startswith("together:"), "model_name must start with 'together:'"
        super().__init__(model_name, max_retries, wait_seconds, temperature, seed, model_path)
        self.client = together.Together(api_key=os.getenv("TOGETHER_API_KEY"))

    def _call_api(self, messages_api_format: list[dict]) -> str:
        # https://github.com/togethercomputer/together-python
        # Remove the "together:" prefix before setting up the client
        completion = self.client.chat.completions.create(
            model=self.model_name[len("together:") :],
            messages=messages_api_format,
            temperature=self.temperature,
            seed=self.seed,
        )
        return completion.choices[0].message.content


class AnthropicChat(CommonLLMChat):
    SUPPORTED_LLM_NAMES: list[str] = [
        "claude-3-5-haiku-20241022",
        "claude-3-5-sonnet-20241022",
    ]

    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        model_path: str | None = None,
    ):
        super().__init__(model_name, max_retries, wait_seconds, temperature, seed, model_path)
        self.client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

    def _call_api(self, messages_api_format: list[dict]) -> str:
        # https://docs.anthropic.com/en/api/migrating-from-text-completions-to-messages
        response = self.client.messages.create(
            model=self.model_name,
            messages=messages_api_format,
            temperature=self.temperature,
        )
        return response.content[0].text


class OllamaChat(CommonLLMChat):
    SUPPORTED_LLM_NAMES: list[str] = [
        "ollama:llama3.2:1b",
    ]

    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        model_path: str | None = None,
    ):
        assert model_name.startswith("ollama:"), "model_name must start with 'ollama:'"
        super().__init__(model_name, max_retries, wait_seconds, temperature, seed, model_path)
        self.ollama_headers: dict = {}
        self.client = ollama.Client(
            host="http://localhost:11434",
            headers=self.ollama_headers,
        )

    def _convert_conv_to_api_format(self, conv: Conversation) -> list[dict]:
        """
        Converts conv into the following format and calls the Ollama client.
        ```
        [
            {"role": role1, "content": text1},
            {"role": role2, "content": text2},
            {"role": role3, "content": text3},
        ]
        ```
        """
        formatted_messages = []
        for message in conv.messages:
            for content in message.content:
                match content:
                    case ContentTextMessage(text=text):
                        formatted_messages.append({"role": message.role, "content": text})
                    # TODO figure out image parsing
        return formatted_messages

    def _call_api(self, messages_api_format: list[dict]) -> str:
        options = dict(
            temperature=self.temperature,
        )
        # Remove the "ollama:" prefix before setting up the client
        response = self.client.chat(
            model=self.model_name[len("ollama:") :],
            messages=messages_api_format,
            options=options,
        )
        return response.message.content


class LocalLlamaChat(LLMChat):
    SUPPORTED_LLM_NAMES: list[str] = [
        "meta-llama/Llama-3.1-8B-Instruct",
        "meta-llama/Llama-3.2-3B-Instruct",
        "meta-llama/Llama-3.2-11B-Vision-Instruct",
    ]

    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        model_path: str | None = None,
    ):
        super().__init__(model_name, max_retries, wait_seconds, temperature, seed, model_path)

        # Use local model if provided
        model_path_to_use = self.model_path or self.model_name
        self.model = MllamaForConditionalGeneration.from_pretrained(
            model_path_to_use,
            torch_dtype=torch.bfloat16,
            device_map="auto",
        )
        self.processor = AutoProcessor.from_pretrained(model_path_to_use)

    def generate_response(self, conv: Conversation) -> str:
        # Take out images from messages
        images: list[Image.Image] = []
        for message in conv.messages:
            for content in message.content:
                match content:
                    case ContentImageMessage(image=image):
                        images.append(image)

        # Process text and images
        input_text = self.processor.apply_chat_template(conv.messages, add_generation_prompt=True)
        inputs = self.processor(
            images,
            input_text,
            add_special_tokens=False,
            return_tensors="pt",
        ).to(self.model.device)

        # Generate and decode
        outputs = self.model.generate(
            **inputs, temperature=self.temperature, max_new_tokens=1024
        )  # shape (1, output_length)
        decoded_output = self.processor.decode(outputs[0])
        user_assistant_alternate_messages: list[str] = decoded_output.split("assistant<|end_header_id|>")
        latest_assistant_message: str = (
            user_assistant_alternate_messages[-1].strip().rstrip("<|eot_id|>")
            if user_assistant_alternate_messages
            else ""
        )
        return latest_assistant_message


SUPPORTED_LLM_NAMES: list[str] = (
    OpenAIChat.SUPPORTED_LLM_NAMES
    + TogetherChat.SUPPORTED_LLM_NAMES
    + AnthropicChat.SUPPORTED_LLM_NAMES
    + OllamaChat.SUPPORTED_LLM_NAMES
    + LocalLlamaChat.SUPPORTED_LLM_NAMES
)


def get_llm(model_name: str, model_kwargs: dict) -> LLMChat:
    match model_name:
        case model_name if model_name in OpenAIChat.SUPPORTED_LLM_NAMES:
            return OpenAIChat(model_name=model_name, **model_kwargs)
        case model_name if model_name in TogetherChat.SUPPORTED_LLM_NAMES:
            return TogetherChat(model_name=model_name, **model_kwargs)
        case model_name if model_name in AnthropicChat.SUPPORTED_LLM_NAMES:
            return AnthropicChat(model_name=model_name, **model_kwargs)
        case model_name if model_name in OllamaChat.SUPPORTED_LLM_NAMES:
            return OllamaChat(model_name=model_name, **model_kwargs)
        case model_name if model_name in LocalLlamaChat.SUPPORTED_LLM_NAMES:
            # LocalLlama models do not support temperature=0, so we set it to 0.01 or higher
            if "temperature" in model_kwargs:
                model_kwargs["temperature"] = max(0.01, model_kwargs["temperature"])
            return LocalLlamaChat(model_name=model_name, **model_kwargs)
        case _:
            raise ValueError(f"Model name {model_name} must be one of {SUPPORTED_LLM_NAMES}.")

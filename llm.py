import abc
import base64
import io
import os

import openai
import torch
from PIL import Image
from tenacity import retry, stop_after_attempt, wait_fixed
from transformers import AutoProcessor, MllamaForConditionalGeneration

from data import ContentImageMessage, ContentTextMessage, Conversation


class LLMChat(abc.ABC):
    def __init__(self, max_retries: int, wait_seconds: int, temperature: float, seed: int):
        """
        Initialize the LLM chat object.

        Args:
            max_retries (int): Max number of API calls allowed before giving up.
            wait_seconds (int): Number of seconds to wait between API calls.
            temperature (float): Temperature parameter for sampling.
            seed (int): Seed for random number generator, passed to the model if applicable.
        """
        self.max_retries = max_retries
        self.wait_seconds = wait_seconds
        self.temperature = temperature
        self.seed = seed

    @abc.abstractmethod
    def generate_response(self, conv: Conversation) -> str:
        """
        Generate a response for the conversation.

        Args:
            conv (Conversation): The conversation object.

        Returns:
            str: The response generated by the model.
        """
        pass


class OpenAIChat(LLMChat):
    def __init__(
        self,
        model_name: str,
        max_retries: int,
        wait_seconds: int,
        temperature: float,
        seed: int,
        stream_generations: bool,
    ):
        """
        Args:
            model_name (str): The model name to use.
            stream_generations (bool): Flag to enable streaming generations.
        """
        super().__init__(max_retries, wait_seconds, temperature, seed)
        self.model_name = model_name
        self.stream_generations = stream_generations
        self.openai_client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    @staticmethod
    def pil_to_base64(image: Image.Image) -> str:
        buffered = io.BytesIO()
        image.save(buffered, format="JPEG")
        return base64.b64encode(buffered.getvalue()).decode("utf-8")

    def generate_response(self, conv: Conversation) -> str:
        # Convert conv to OpenAI format
        openai_conv = []
        for message in conv.messages:
            for content in message.content:
                match content:
                    case ContentTextMessage(text=text):
                        openai_conv.append({"role": message.role, "content": [{"type": "text", "text": text}]})
                    case ContentImageMessage(image=image):
                        base64_image = OpenAIChat.pil_to_base64(image)
                        openai_conv.append(
                            {
                                "role": message.role,
                                "content": [{
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                                }],
                            }
                        )

        # Wrap retry params inside generate_response
        @retry(stop=stop_after_attempt(self.max_retries), wait=wait_fixed(self.wait_seconds))
        def _call_api(conv: Conversation) -> str:
            completion = self.openai_client.chat.completions.create(
                model=self.model_name,
                messages=openai_conv,
                temperature=self.temperature,
                seed=self.seed,
                stream=self.stream_generations,
            )
            return completion if self.stream_generations else completion.choices[0].message.content

        return _call_api(conv)


class LlamaChat(LLMChat):
    def __init__(self, model_path: str, max_retries: int, wait_seconds: int, temperature: float, seed: int):
        """
        Args:
            model_path (str): Huggingface hub model path or local model path.
        """
        super().__init__(max_retries, wait_seconds, temperature, seed)
        self.model_path = model_path

        self.model = MllamaForConditionalGeneration.from_pretrained(
            self.model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
        )
        self.processor = AutoProcessor.from_pretrained(self.model_path)

    def generate_response(self, conv: Conversation) -> str:
        # Take out images from messages
        images: list[Image.Image] = []
        for message in conv.messages:
            for content in message.content:
                match content:
                    case ContentImageMessage(image=image):
                        images.append(image)

        # Process text and images
        input_text = self.processor.apply_chat_template(conv.messages, add_generation_prompt=True)
        inputs = self.processor(
            images,
            input_text,
            add_special_tokens=False,
            return_tensors="pt",
        ).to(self.model.device)

        # Generate and decode
        outputs = self.model.generate(
            **inputs, temperature=self.temperature, max_new_tokens=1024
        )  # shape (1, output_length)
        decoded_output = self.processor.decode(outputs[0])
        user_assistant_alternate_messages: list[str] = decoded_output.split("assistant<|end_header_id|>")
        latest_assistant_message: str = (
            user_assistant_alternate_messages[-1].strip().rstrip("<|eot_id|>")
            if user_assistant_alternate_messages
            else ""
        )
        return latest_assistant_message
